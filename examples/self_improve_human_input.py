"""
Example: Self-improvement with human in the loop.

This example demonstrates how to improve BQ answers by:
1. Getting initial feedback on an answer
2. Extracting probing questions from feedback
3. Using user's real answers to probing questions to improve the answer
4. Re-evaluating until Strong Hire or max iterations

Two modes:
- run_interactive(): Prompts user for input via terminal
- run_with_predefined_answers(): Uses predefined answers for testing
"""

import os
import sys
import asyncio
from pathlib import Path

_project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

from advance.self_improve import HumanInLoopImprove
from utils import Colors

# Path to feedback file with initial answer (generated by solve_conflict.py)
FEEDBACK_FILE = Path(_project_root) / "feedbacks" / "20251129-Leaning_No_Hire_4.md"

# Predefined answers for testing (simulates user input)
# These answers match the NEW targeted probing questions format
PREDEFINED_PROBING_ANSWERS = [
    # Iteration 1 answers (matching new targeted probing questions)
    [
        # Q: Ownership: "What was YOUR specific decision that changed the outcome?"
        """My specific decision was to propose a 60/40 resource split instead of the 50/50
        that everyone was debating. I calculated this myself by analyzing both projects'
        critical paths. I presented this to my manager first, got her buy-in, then brought
        it to the group meeting as a concrete proposal rather than opening another debate.""",

        # Q: Reflection: "What mistake did you make, and what would you do differently?"
        """My mistake was not talking to the infrastructure lead James privately before
        the meeting. He felt blindsided when I presented the data showing his project
        could slip. He got defensive and almost derailed the discussion. Next time,
        I would have a 1:1 with each stakeholder before any group meeting to understand
        their concerns and give them a heads up on the data.""",

        # Q: Metrics: "What specific metrics can you share about YOUR contribution?"
        """I personally tracked three metrics weekly: sprint velocity, revenue pipeline,
        and system uptime. After 8 weeks, product shipped their MVP generating $1.2M
        in pipeline (vs $800K target). Infrastructure improved p99 latency from 450ms
        to 180ms. I built the dashboard myself and presented it in our weekly standup.""",

        # Q: Impact: "Can you describe a situation where your actions directly influenced the outcome?"
        """When the product director pushed for 80/20 split, I was the one who pushed back.
        I said 'If infrastructure fails, we lose everything' and showed her the outage
        projections I had prepared. She backed down. That was uncomfortable but my
        analysis directly changed the final decision from 80/20 to 60/40.""",

        # Q: Clarity: "How did you ensure stakeholders were aligned on the data?"
        """I created a one-page summary with three scenarios and sent it 24 hours before
        the meeting. I asked each lead to review and flag any data they disagreed with.
        James from infrastructure caught an error in my capacity projection - I had
        underestimated their load by 10%. I corrected it before the meeting, which
        actually strengthened his trust in the process.""",

        # Q: Personal Contribution: "What role did you play in shaping the final decision?"
        """I was the one who proposed the hybrid 60/40 split and the decision criteria.
        I also volunteered to own the weekly tracking. My manager later told me the VP
        specifically mentioned that my structured approach turned a deadlocked situation
        into a resolution in under a week. Three months later, James came to me first
        when another conflict arose - that told me I'd earned credibility.""",
    ],
]


async def test_with_predefined_answers():
    """Test human-in-loop improvement with predefined answers"""
    print("=" * 80)
    print("TESTING HUMAN-IN-LOOP SELF-IMPROVEMENT (Predefined Answers)")
    print("=" * 80)

    improver = HumanInLoopImprove(
        feedback_file=FEEDBACK_FILE,
        level="Junior-Mid",
        max_iterations=2,
    )

    print(f"\nFeedback file: {FEEDBACK_FILE}")
    print(f"\nOriginal Question: {await improver.question()}")
    print(f"\nOriginal Answer:\n{await improver.answer()}")

    result = await improver.run_with_predefined_answers(PREDEFINED_PROBING_ANSWERS)

    print("\n" + "=" * 80)
    print("FINAL RESULT")
    print("=" * 80)
    print(f"Status: {result['status']}")
    print(f"Iterations: {result['iterations']}")
    print(f"\nFinal Answer:\n{result['final_answer']}")

    # Print final feedback
    if result['feedback_history']:
        last_feedback = result['feedback_history'][-1]
        print(f"\n{'='*40}")
        print("Final Feedback:")
        print(f"{'='*40}")
        print(Colors.feedback(last_feedback['feedback']))
        print(f"\n{'='*40}")
        print("Final Red Flag Check:")
        print(f"{'='*40}")
        print(Colors.feedback(last_feedback['red_flag']))

    return result


async def test_interactive():
    """Test human-in-loop improvement with real user input"""
    print("=" * 80)
    print("TESTING HUMAN-IN-LOOP SELF-IMPROVEMENT (Interactive)")
    print("=" * 80)

    improver = HumanInLoopImprove(
        feedback_file=FEEDBACK_FILE,
        level="Junior-Mid",
        max_iterations=3,
    )

    print(f"\nFeedback file: {FEEDBACK_FILE}")
    print(f"\nOriginal Question: {await improver.question()}")
    print(f"\nOriginal Answer:\n{await improver.answer()}")

    result = await improver.run_interactive()

    print("\n" + "=" * 80)
    print("FINAL RESULT")
    print("=" * 80)
    print(f"Status: {result['status']}")
    print(f"Iterations: {result['iterations']}")
    print(f"\nFinal Answer:\n{result['final_answer']}")

    return result


async def main():
    # Run with predefined answers for testing
    await test_with_predefined_answers()

    # Uncomment to run interactive mode:
    # await test_interactive()


if __name__ == "__main__":
    asyncio.run(main())
